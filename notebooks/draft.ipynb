{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brightly', 'jumping', 'jellybeans', 'became', 'jubilant', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Download necessary NLTK data files (only need to run once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    # Tokenize the sentence into words using word_tokenize\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # Get POS tags for each token\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for token, (word, pos) in zip(tokens, pos_tags):\n",
    "        # Add POS tag and the whole word to the output list\n",
    "        output.extend([token])\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage with the original sentence\n",
    "sentence = \"Brightly jumping jellybeans became jubilant.\"\n",
    "result = process_sentence(sentence)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['豆', '車', '通', '道', '郎', '郡', '部', '都', '野', '金', '鉄', '長', '門', '関', '青', '音', '風', '馬', '高', '가', '고', '국', '기', '김', '나', '는', '다', '대', '도', '독', '라', '로', '리', '마', '부', '사', '서', '스', '시', '아', '에', '을', '의', '이', '일', '전', '제', '조', '주', '지', '하', '한']\n"
     ]
    }
   ],
   "source": [
    "RARE_CHARS_RAW = \"\"\"\n",
    "豆 948\n",
    "車 949\n",
    "通 950\n",
    "道 951\n",
    "郎 952\n",
    "郡 953\n",
    "部 954\n",
    "都 955\n",
    "野 956\n",
    "金 957\n",
    "鉄 958\n",
    "長 959\n",
    "門 960\n",
    "関 961\n",
    "青 962\n",
    "音 963\n",
    "風 964\n",
    "馬 965\n",
    "高 966\n",
    "가 967\n",
    "고 968\n",
    "국 969\n",
    "기 970\n",
    "김 971\n",
    "나 972\n",
    "는 973\n",
    "다 974\n",
    "대 975\n",
    "도 976\n",
    "독 977\n",
    "라 978\n",
    "로 979\n",
    "리 980\n",
    "마 981\n",
    "부 982\n",
    "사 983\n",
    "서 984\n",
    "스 985\n",
    "시 986\n",
    "아 987\n",
    "에 988\n",
    "을 989\n",
    "의 990\n",
    "이 991\n",
    "일 992\n",
    "전 993\n",
    "제 994\n",
    "조 995\n",
    "주 996\n",
    "지 997\n",
    "하 998\n",
    "한 999\n",
    "\"\"\".strip().split()\n",
    "\n",
    "vals = [v for v in RARE_CHARS_RAW if not v.isnumeric()]\n",
    "print(repr(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
